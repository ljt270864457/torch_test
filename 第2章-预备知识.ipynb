{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据操作\n",
    "\n",
    "## 创建张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 1.4058e+02, -2.4394e-41,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建空的张量\n",
    "x = torch.empty(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3320, 0.8642, 0.0843],\n",
       "        [0.5064, 0.8842, 0.3225],\n",
       "        [0.2590, 0.9896, 0.6651],\n",
       "        [0.9001, 0.1219, 0.7102],\n",
       "        [0.5948, 0.1458, 0.7478]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建随机张量\n",
    "x = torch.rand(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 3])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5, 3])\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 0.7420,  1.0910,  0.7244],\n",
      "        [ 0.3394, -0.2583,  1.8374],\n",
      "        [ 0.2697, -1.2103,  0.2198],\n",
      "        [-0.3692, -1.1297,  0.2661],\n",
      "        [ 0.1897,  0.2342,  1.5984]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 返回的tensor默认具有相同的torch.dtype和torch.device\n",
    "x = x.new_ones(5, 3, dtype=torch.float64)\n",
    "print(x)\n",
    "x = torch.randn_like(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4928, 0.9540, 0.7717],\n",
      "        [0.7873, 0.3879, 0.0653],\n",
      "        [0.3126, 0.9069, 0.6232],\n",
      "        [0.5599, 0.2179, 0.1127],\n",
      "        [0.4148, 0.3433, 0.9053]])\n",
      "tensor([[0.0507, 0.7102, 0.4789],\n",
      "        [0.0714, 0.0100, 0.2006],\n",
      "        [0.2647, 0.0670, 0.0339],\n",
      "        [0.9937, 0.9781, 0.5189],\n",
      "        [0.6912, 0.0874, 0.6480]])\n",
      "tensor([[0.5435, 1.6642, 1.2506],\n",
      "        [0.8586, 0.3979, 0.2659],\n",
      "        [0.5774, 0.9739, 0.6570],\n",
      "        [1.5536, 1.1961, 0.6316],\n",
      "        [1.1060, 0.4307, 1.5533]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "y = torch.rand(5, 3)\n",
    "print(y)\n",
    "z3 = torch.empty_like(x)\n",
    "z1 = x+y\n",
    "z2 = torch.add(x, y)\n",
    "torch.add(x, y, out=z3)\n",
    "assert z1.equal(z2) and z2.equal(z3)\n",
    "print(z3)\n",
    "# inplace\n",
    "x.add_(y)\n",
    "assert x.equal(z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5435, 1.6642, 1.2506])\n",
      "tensor([1.5435, 2.6642, 2.2506])\n",
      "tensor([[1.5435, 2.6642, 2.2506],\n",
      "        [0.8586, 0.3979, 0.2659],\n",
      "        [0.5774, 0.9739, 0.6570],\n",
      "        [1.5536, 1.1961, 0.6316],\n",
      "        [1.1060, 0.4307, 1.5533]])\n"
     ]
    }
   ],
   "source": [
    "# 我们还可以使用类似NumPy的索引操作来访问Tensor的一部分，需要注意的是：索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改。\n",
    "y = z1[0, :]\n",
    "print(y)\n",
    "y += 1\n",
    "print(y)\n",
    "# z1的值也变了\n",
    "print(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5774, 0.9739, 0.6570],\n",
       "        [0.8586, 0.3979, 0.2659]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 根据指定维度筛选某几行\n",
    "indexs = torch.tensor([2, 1])\n",
    "torch.index_select(z1, 0, indexs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n",
      "tensor([[ 0,  5, 10, 15]])\n",
      "tensor([[ 8,  5, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "# gather 是按轴向(axis)，按照指定行取数据 \n",
    "a = torch.arange(0,16).view(4,4)\n",
    "print(a)\n",
    "index = torch.LongTensor([[0,1,2,3]])\n",
    "print(a.gather(0,index))\n",
    "# ==========\n",
    "index = torch.LongTensor([[2,1,2,2]])\n",
    "print(a.gather(0,index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5435, 2.6642, 2.2506, 0.8586, 0.3979, 0.2659, 0.5774, 0.9739, 0.6570,\n",
       "        1.5536, 1.1961, 0.6316, 1.1060, 0.4307, 1.5533])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 根据mask进行选择\n",
    "torch.masked_select(z1, z1 > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [0., 1., 1.]])\n",
      "tensor([[0, 0],\n",
      "        [0, 1],\n",
      "        [0, 2],\n",
      "        [1, 0],\n",
      "        [1, 1],\n",
      "        [1, 2],\n",
      "        [2, 1],\n",
      "        [2, 2]])\n"
     ]
    }
   ],
   "source": [
    "# 非零元素的下标\n",
    "a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]\n",
    "a = torch.bernoulli(a)\n",
    "print(a)\n",
    "print(torch.nonzero(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修改形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n",
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7]],\n",
      "\n",
      "        [[ 8,  9, 10, 11],\n",
      "         [12, 13, 14, 15]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(0, 16)\n",
    "print(a.view(16))\n",
    "print(a.view(4, -1))\n",
    "print(a.view(2, 2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12],\n",
      "        [13, 14, 15, 16]])\n"
     ]
    }
   ],
   "source": [
    "# view也是浅拷贝，原始数据修改也会导致view之后的数据更改\n",
    "b = a.view(4, -1)\n",
    "a += 1\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# 如果想创建副本，建议使用clone的方式进行创建\n",
    "x = torch.tensor([1,2,3])\n",
    "x_cp = x.clone()\n",
    "print(id(x)==id(x_cp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性代数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|函数|功能|\n",
    "|---|---|\n",
    "|trace|对角线元素之和(矩阵的迹)|\n",
    "|diag|对角线元素|\n",
    "|triu/tril|矩阵的上三角/下三角，可指定偏移量|\n",
    "|mm/bmm|矩阵乘法，batch的矩阵乘法|\n",
    "|addmm/addbmm/addmv/addr/baddbmm..|矩阵运算|\n",
    "|t|转置|\n",
    "|dot/cross|向量内积/外积|\n",
    "|inverse|求逆矩阵|\n",
    "|svd|奇异值分解|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4393, 0.8236]])\n",
      "tensor([[ 1.6200,  1.7029],\n",
      "        [-0.8190, -0.0148]])\n",
      "====================\n",
      "tensor(1.6053)\n",
      "====================\n",
      "tensor([ 1.6200, -0.0148])\n",
      "====================\n",
      "tensor([[ 1.6200,  1.7029],\n",
      "        [ 0.0000, -0.0148]])\n",
      "====================\n",
      "tensor([[ 1.6200,  0.0000],\n",
      "        [-0.8190, -0.0148]])\n",
      "====================\n",
      "tensor([[ 1.6200, -0.8190],\n",
      "        [ 1.7029, -0.0148]])\n",
      "====================\n",
      "tensor([[-0.0108, -1.2422],\n",
      "        [ 0.5975,  1.1818]])\n",
      "====================\n",
      "torch.return_types.svd(\n",
      "U=tensor([[-0.9679,  0.2514],\n",
      "        [ 0.2514,  0.9679]]),\n",
      "S=tensor([2.4240, 0.5655]),\n",
      "V=tensor([[-0.7318, -0.6815],\n",
      "        [-0.6815,  0.7318]]))\n",
      "====================\n",
      "tensor([[0.0371, 0.7360]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,2)\n",
    "b = torch.randn(2,2)\n",
    "print(a)\n",
    "print(b)\n",
    "print('='*20)\n",
    "print(b.trace())\n",
    "print('='*20)\n",
    "print(b.diag())\n",
    "print('='*20)\n",
    "print(b.triu())\n",
    "print('='*20)\n",
    "print(b.tril())\n",
    "print('='*20)\n",
    "print(b.t())\n",
    "print('='*20)\n",
    "print(b.inverse())\n",
    "print('='*20)\n",
    "print(b.svd())\n",
    "print('='*20)\n",
    "# 矩阵乘法\n",
    "print(torch.matmul(a,b))\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor和numpy互转"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.36351752 -0.13347013]\n",
      " [-0.76513656 -0.35483407]\n",
      " [-0.43934627  0.86029124]]\n",
      "tensor([[-0.3635, -0.1335],\n",
      "        [-0.7651, -0.3548],\n",
      "        [-0.4393,  0.8603]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(3,2)\n",
    "b = torch.from_numpy(a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动求梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 概念\n",
    "    - 如果需要对张量进行梯度传播，那么需要将. **requires_grad=True** ,完成计算后可以调用 **backup()** 进行梯度计算，梯度累计到 **.grad** 属性 \n",
    "    - 如果不想被追踪，可以调用 **.detach()**，这样梯度就不会继续传递。此外还可以使用 **with torch.no_grad()** 包裹起来（通常在模型评估时使用）\n",
    "    - **Function** 是一个很重要的类，**Tensor** 与 **Function** 可以构建DAG，每个tensor都有一个 **.grad_fn** 属性，该属性记录是从哪计算出来的方便调试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "None\n",
      "==================================================\n",
      "<AddBackward0 object at 0x000001891BB47148>\n",
      "True False\n"
     ]
    }
   ],
   "source": [
    "# 自己定义的张量grad_fn为None,属于叶子结点\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "print(x.grad_fn)\n",
    "print('='*50)\n",
    "y = torch.add(x, 1)\n",
    "print(y.grad_fn)\n",
    "print(x.is_leaf, y.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.5831, -1.1134],\n",
      "        [-0.8542, -1.6061]])\n",
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x000001891BB478C8>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "print(a)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)  # False\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度\n",
    "\n",
    "- 不允许张量对张量求导，只允许标量对张量求导，求导结果和自变量同型的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.5000, 5.5000],\n",
      "        [5.5000, 5.5000]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 梯度是累加的，所以通常要在反向传播之前把梯度清0\n",
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "\n",
    "out3 = x.sum()\n",
    "x.grad.data.zero_()\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
    "y = 2 * x\n",
    "z = y.view(2, 2)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000, 0.2000, 0.0200, 0.0020])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([[1.0, 0.1], [0.01, 0.001]], dtype=torch.float)\n",
    "z.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设 x 经过一番计算得到 y，那么 y.backward(w) 求的不是 y 对 x 的导数，而是 l = torch.sum(y*w) 对 x 的导数。w 可以视为 y 的各分量的权重，也可以视为遥远的损失函数 l 对 y 的偏导数。也就是说，不一定需要从计算图最后的节点 y 往前反向传播，从中间某个节点 n 开始传也可以，只要你能把损失函数 l 关于这个节点的导数 dl/dn 记录下来，n.backward(dl/dn) 照样能往前回传，正确地计算出损失函数 l 对于节点 n 之前的节点的导数。特别地，若 y 为标量，w 取默认值 1.0，才是按照我们通常理解的那样，求 y 对 x 的导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1455, -0.8896,  0.3153], requires_grad=True)\n",
      "tensor([ 1.2853, -0.7328,  0.2472], requires_grad=True)\n",
      "tensor([1.4532, 0.6524, 0.1970], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "x = Variable(torch.randn(3), requires_grad=True)\n",
    "y = Variable(torch.randn(3), requires_grad=True)\n",
    "z = Variable(torch.randn(3), requires_grad=True)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "\n",
    "t = x + y\n",
    "# (x+y).t * z  \n",
    "l = t.dot(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4532, 0.6524, 0.1970])\n",
      "tensor([1.4532, 0.6524, 0.1970])\n",
      "tensor([1.4532, 0.6524, 0.1970], requires_grad=True)\n",
      "tensor([ 1.1399, -1.6224,  0.5625])\n",
      "tensor([ 1.1399, -1.6224,  0.5625], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "l.backward(retain_graph=True)\n",
    "print(x.grad)\n",
    "print(y.grad) # x.grad = y.grad = z\n",
    "print(z)\n",
    "\n",
    "print(z.grad) # z.grad = t = x + y\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4532, 0.6524, 0.1970])\n",
      "tensor([1.4532, 0.6524, 0.1970])\n"
     ]
    }
   ],
   "source": [
    "# 上下等效\n",
    "x.grad.data.zero_()\n",
    "y.grad.data.zero_()\n",
    "z.grad.data.zero_()\n",
    "\n",
    "t.backward(z)\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(1., grad_fn=<PowBackward0>) True\n",
      "tensor(1.) False\n",
      "tensor(2., grad_fn=<AddBackward0>) True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y1 = x ** 2 \n",
    "with torch.no_grad():\n",
    "    y2 = x ** 3\n",
    "y3 = y1 + y2\n",
    "\n",
    "print(x.requires_grad)\n",
    "print(y1, y1.requires_grad) # True\n",
    "print(y2, y2.requires_grad) # False\n",
    "print(y3, y3.requires_grad) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 779.75678,
   "position": {
    "height": "40px",
    "left": "824.208px",
    "right": "20px",
    "top": "271.986px",
    "width": "613.75px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
